apiVersion: batch/v1
kind: Job
metadata:
  name: carbon-v2-preprocess-retry
  namespace: biodiversity
spec:
  # Only the 6 datasets that didn't complete:
  #   0 -> vulnerable_c_total_2024      (orig zip 02)
  #   1 -> irrecoverable_c_total_2022   (orig zip 05)
  #   2 -> manageable_c_total_2018      (orig zip 08)
  #   3 -> irrecoverable_c_total_2010   (orig zip 09, was failing)
  #   4 -> vulnerable_c_total_2010      (orig zip 10)
  #   5 -> manageable_c_total_2010      (orig zip 11)
  completions: 6
  parallelism: 3
  completionMode: Indexed
  backoffLimit: 1
  ttlSecondsAfterFinished: 86400
  template:
    spec:
      priorityClassName: opportunistic
      restartPolicy: Never
      volumes:
      - name: rclone-config
        secret:
          secretName: rclone-config
      containers:
      - name: preprocess
        image: ghcr.io/boettiger-lab/datasets:latest
        imagePullPolicy: Always
        env:
        - name: AWS_ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
              name: aws
              key: AWS_ACCESS_KEY_ID
        - name: AWS_SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
              name: aws
              key: AWS_SECRET_ACCESS_KEY
        - name: AWS_S3_ENDPOINT
          value: rook-ceph-rgw-nautiluss3.rook
        - name: AWS_PUBLIC_ENDPOINT
          value: s3-west.nrp-nautilus.io
        - name: AWS_HTTPS
          value: 'false'
        - name: AWS_VIRTUAL_HOSTING
          value: 'FALSE'
        - name: GDAL_DATA
          value: /usr/share/gdal
        volumeMounts:
        - name: rclone-config
          mountPath: /root/.config/rclone
          readOnly: true
        resources:
          requests:
            cpu: '4'
            memory: 16Gi
            ephemeral-storage: 120Gi
          limits:
            cpu: '4'
            memory: 16Gi
            ephemeral-storage: 120Gi
        command:
        - bash
        - -c
        - |
          set -e

          ZIPS=(
            "02_Vulnerable_Carbon_Total_v1a_300m_2024.zip:vulnerable_c_total_2024"
            "05_Irrecoverable_Carbon_Total_v1a_300m_2022.zip:irrecoverable_c_total_2022"
            "08_Manageable_Carbon_Total_v1a_300m_2018.zip:manageable_c_total_2018"
            "09_Irrecoverable_Carbon_Total_v1a_300m_2010.zip:irrecoverable_c_total_2010"
            "10_Vulnerable_Carbon_Total_v1a_300m_2010.zip:vulnerable_c_total_2010"
            "11_Manageable_Carbon_Total_v1a_300m_2010.zip:manageable_c_total_2010"
          )

          ENTRY="${ZIPS[$JOB_COMPLETION_INDEX]}"
          ZIP_NAME="${ENTRY%%:*}"
          OUTPUT_STEM="${ENTRY##*:}"

          ZENODO_URL="https://zenodo.org/records/17645053/files/${ZIP_NAME}?download=1"
          WORKDIR="/tmp/carbon_${JOB_COMPLETION_INDEX}"
          mkdir -p "${WORKDIR}"
          cd "${WORKDIR}"

          # Locate PROJ database
          PROJ_DB=$(find /usr /opt -name "proj.db" 2>/dev/null | head -1)
          if [ -n "${PROJ_DB}" ]; then
            export PROJ_DATA="$(dirname ${PROJ_DB})"
            export PROJ_LIB="$(dirname ${PROJ_DB})"
          fi

          echo "=== Processing index ${JOB_COMPLETION_INDEX}: ${ZIP_NAME} ==="
          echo "Output: ${OUTPUT_STEM}"

          # Download with progress
          curl -L --retry 5 --retry-delay 15 --retry-all-errors \
            -o "${ZIP_NAME}" "${ZENODO_URL}"
          echo "Downloaded: $(du -h ${ZIP_NAME})"

          echo "Unzipping..."
          unzip -q "${ZIP_NAME}"
          rm "${ZIP_NAME}"

          INPUT_TIF=$(find "${WORKDIR}" -name "*.tif" | head -1)
          if [ -z "${INPUT_TIF}" ]; then
            echo "ERROR: No .tif found after unzip!"
            find "${WORKDIR}" -type f
            exit 1
          fi
          echo "Source TIF: ${INPUT_TIF} ($(du -h ${INPUT_TIF}))"

          # Verify GDAL can open the file; diagnose without failing
          echo "Running gdalinfo..."
          gdalinfo "${INPUT_TIF}" 2>&1 | head -20 || echo "gdalinfo warning (continuing)"

          # Two-step COG conversion to avoid multi-IFD / BigTIFF-pointer issues:
          # Step 1: strip to a clean single-band flat GeoTIFF
          echo "Step 1: extracting band 1 to intermediate GeoTIFF..."
          INTERMEDIATE="${WORKDIR}/intermediate.tif"
          gdal_translate \
            -b 1 \
            -of GTiff \
            -co BIGTIFF=YES \
            -co COMPRESS=NONE \
            "${INPUT_TIF}" \
            "${INTERMEDIATE}"
          rm "${INPUT_TIF}"
          echo "Intermediate: $(du -h ${INTERMEDIATE})"

          # Step 2: convert intermediate to COG
          echo "Step 2: converting to COG..."
          COG_OUT="${WORKDIR}/${OUTPUT_STEM}.tif"
          gdal_translate \
            -of COG \
            -co COMPRESS=ZSTD \
            -co PREDICTOR=YES \
            -co OVERVIEW_RESAMPLING=AVERAGE \
            -co BIGTIFF=YES \
            "${INTERMEDIATE}" \
            "${COG_OUT}"
          rm "${INTERMEDIATE}"
          echo "COG: $(du -h ${COG_OUT})"

          echo "Uploading to S3..."
          rclone copy \
            "${COG_OUT}" \
            "nrp:public-carbon/v2/cogs/" \
            --progress

          echo "=== Done: nrp:public-carbon/v2/cogs/${OUTPUT_STEM}.tif ==="
